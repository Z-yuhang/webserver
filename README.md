# WebServer项目笔记

## 线程同步

* **线程同步**：当有一个线程在对内存进行操作时，其他线程都不可以对这个内存地址进行操作，直到该线程完成操作， 其他线程才能对该内存地址进行操作，而其他线程又处于等待状态。目的是避免同时操作变量，产生冲突！

* **信号量**：信号量是一种特殊的变量，只能取自然数值，支持等待(P)和信号(V)两种操作。信号量大于0时，等待信号量的线程被唤醒；

* **互斥锁**：用于保护关键代码段，确保独占式访问。当进入关键代码段，获得互斥锁将其加锁，执行完关键代码段后解锁，唤醒等待该互斥锁的线程；

* **条件变量**：用于在线程间同步共享数据的值,条件变量提供了一种线程间的通知机制,当某个共享数据达到某个值时,唤醒等待的线程；

## 线程池

* **事件处理模式**
  * Reactor模式：主线程(**I/O处理单元**)只负责监听文件描述符上是否有事件发生，有的话立即通知工作线程(**逻辑单元** )，读写数据、接受新连接及处理客户请求均在工作线程中完成。（一般由同步I/O实现）
  * Proactor模式中，主线程和内核负责处理读写数据、接受新连接等I/O操作，工作线程仅负责业务逻辑，如处理客户请求。（一般由异步I/O实现）
* **同步I/O模拟Proactor模式**

* **线程池**：池的概念都是基于以空间资源换取时间的思想，初始化一定数量的线程作为工作线程，在**程序运行期间工作线程不会被关闭，以避免初始化线程和关闭线程的操作**，当有任务出现时，工作线程获取任务开始工作；

* **线程的工作逻辑**：没有任务时线程被sem信号量m_queuestat.wait()阻塞，直到任务队列中有新的任务；

* **线程池中工作线程处理完一个任务后的状态**：如果请求队列为空，则该线程进入线程池中等待；若不为空，则该线程跟其他线程一起进行任务的竞争；

## 定时器处理非活动连接

* **定时事件**：是指固定一段时间之后触发某段代码，由该段代码处理一个事件。
  * 应用：定时时间到达后，从内核事件表删除事件，并关闭文件描述符，释放连接资源；
* **定时器**：是指利用结构体或其他形式，将多种定时事件进行封装起来。
  * 应用：将定时事件(定期检测非活动连接)与连接资源封装为一个结构体定时器；
* **定时器容器**：是指使用某种容器类数据结构，将上述多个定时器组合起来，便于对定时事件统一管理。
  * 应用：使用**升序链表**管理定时器，将定时器组织串联起来；
  * 其他定时器容器：时间轮，时间堆；

### 定时器(SIGALRM信号)整体思路

* alarm( )函数周期性触发SIGALRM信号
* 信号处理函数利用**管道**通知主循环
* 主循环接收信号(注册epoll事件)，对升序链表上的定时器做处理 (若该段时间内没有交换数据，则将该连接关闭，释放所占用的资源)

## HTTP连接处理

### **HTTP请求报文**

HTTP请求报文由**请求行**(request line)、**请求头部**(header)、**空行**和**请求数据**四个部分组成。

- **请求行**，用来说明请求类型，要访问的资源以及所使用的HTTP版本。GET说明请求类型为GET，POST说明请求类型为POST，该行的最后一部分说明使用的HTTP版本。

- **请求头部**，紧接着请求行（即第一行）之后的部分，用来说明服务器要使用的附加信息。

- - HOST，给出请求资源所在服务器的域名。
  - User-Agent，HTTP客户端程序的信息，该信息由你发出请求使用的浏览器来定义,并且在每个请求中自动发送等。
  - Accept，说明用户代理可处理的媒体类型。
  - Accept-Encoding，说明用户代理支持的内容编码。
  - Accept-Language，说明用户代理能够处理的自然语言集。
  - Content-Type，说明实现主体的媒体类型。
  - Content-Length，说明实现主体的大小。
  - Connection，连接管理，可以是Keep-Alive或close。

- **空行**，请求头部后面的空行是必须的即使第四部分的请求数据为空，也必须有空行。

- **请求数据**也叫主体，可以添加任意的其他数据。

```http
POST / HTTP1.1
Host:www.wrox.com
User-Agent:Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022)
Content-Type:application/x-www-form-urlencoded
Content-Length:40
Connection: Keep-Alive
空行
name=Professional%20Ajax&publisher=Wiley
```

### **HTTP响应报文**

HTTP响应由四个部分组成，分别是：**状态行**、**消息报头**、**空行**和**响应正文**。

- 状态行，由HTTP协议版本号， 状态码， 状态消息 三部分组成。第一行为状态行，（HTTP/1.1）表明HTTP版本为1.1版本，状态码为200，状态消息为OK。
- 消息报头，用来说明客户端要使用的一些附加信息。第二行和第三行为消息报头，Date:生成响应的日期和时间；Content-Type:指定了MIME类型的HTML(text/html),编码类型是UTF-8。
- 空行，消息报头后面的空行是必须的。
- 响应正文，服务器返回给客户端的文本信息。空行后面的html部分为响应正文。

```http
 HTTP/1.1 200 OK
 Date: Fri, 22 May 2009 06:07:21 GMT
 Content-Type: text/html; charset=UTF-8
 空行
 <html>
       <head></head>
       <body>
             <!--body goes here-->
       </body>
</html>
```

## 日志处理

* 同步日志：日志写入函数与工作线程串行执行；
  * 缺点：由于涉及到I/O操作，当单条日志比较大的时候，同步模式会阻塞整个处理流程，服务器所能处理的并发能力将有所下降，尤其是在峰值的时候，写日志可能成为系统的瓶颈
* 异步日志：将所写的日志内容先存入阻塞队列，写线程从阻塞队列中取出内容，写入日志；

## 数据库连接

* 整体思路：使用**链表**和**单例模式**创建数据库连接池，实现对数据库连接资源的复用；

* 注册登录
  * 注册事件处理
  * 登录事件处理
* 处理流程：
  * 载入数据库表：http_conn中，连接数据库时讲数据库中的全部信息读取到一张map中；
  * 提取用户名和密码：对报文(HTTP请求)进行解析，提取用户名和密码；
  * 注册登录：对用户名和密码进行校验
  * 页面跳转

## 压测

### 压测服务器的方式

工具：Webbench

功能：测试处在相同硬件上，不同服务的性能以及不同硬件上同一个服务的运行状况；

输出参数：每秒钟的响应请求数量(QPS)和每秒传输的数据量；

### 压测的基本原理

Webbench 首先 fork 出多个子进程，每个子进程都循环做 web 访问测试。子进程把访问的结果通过pipe 告诉父进程，父进程做最终的统计结果。

### 压测的执行命令

```shell
webbench -c 并发数量 -t 测试时间 URL
```



# 项目运行

## WebServer启动方式

**服务器测试环境**

* 阿里云轻量应用服务器，Ubuntu版本20.04，CPU2核，内存4GB，系统盘60G；
* MySQL版本5.7.37

**浏览器测试环境**

* Windows、Linux均可
* Chrome

**测试前确认已安装MySQL数据库，并且MySQL端口3306已开启**

```sql
-- 打开MySQL，建立webserver数据库和user表
mysql -uroot -p

-- 建立yourdb库
create database webserver;

-- 创建user表
USE webserver;
CREATE TABLE user(
    username varchar(50) NULL,
    passwd varchar(50) NULL
)ENGINE=InnoDB;

-- 添加数据
INSERT INTO user(username, passwd) VALUES('name', 'passwd');
```

**修改部分代码**

```c++
// 在Webmain.cpp中修改数据库的配置信息
string user = "root";
string passwd = "123456";
string databasename = "webserver";

// 在webserver.cpp中第78行修改数据库初始化信息，主要修改IP地址
m_connPool->init("172.19.50.54", m_user, m_passWord, m_databaseName, 3306, m_sql_num, m_close_log);

// 在webserver.cpp中修改文件的根路径
char server_path[200] = "/home/Webserver";
```

**编译项目**

```shell
# 清理之前的编译结果
make clean
# 编译
make server
```

**运行项目**

```shell
# 个性化运行
./server [-p port] [-l LOGWrite] [-m TRIGMode] [-o OPT_LINGER] [-s sql_num] [-t thread_num] [-c close_log] [-a actor_model]
```

以上参数不是非必须，不用全部使用，根据个人情况搭配选用即可.

* -p，自定义端口号
  * 默认9006
* -l，选择日志写入方式，默认同步写入
  * 0，同步写入
  * 1，异步写入
* -m，listenfd和connfd的模式组合，默认使用LT + LT
  * 0，表示使用LT + LT
  * 1，表示使用LT + ET
  * 2，表示使用ET + LT
  * 3，表示使用ET + ET
* -o，优雅关闭连接，默认不使用
  * 0，不使用
  * 1，使用
* -s，数据库连接数量
  * 默认为8
* -t，线程数量
  * 默认为8
* -c，关闭日志，默认打开
  * 0，打开日志
  * 1，关闭日志
* -a，选择反应堆模型，默认Proactor
  * 0，Proactor模型
  * 1，Reactor模型

**服务器访问**

* 在Chrome浏览器中搜索：`服务器IP地址:端口号`向服务端发送请求

## **压力测试**

安装webbench

```shell
# 跳转到 ./stress_test/webbench-1.5文件夹
make 
sudo make install
```

在关闭日志后，使用Webbench对服务器进行压力测试，对listenfd和connfd分别采用ET和LT模式：

测试环境：阿里云轻量应用服务器，Ubuntu版本20.04，CPU2核，内存4GB，系统盘60G；

* Proactor，LT + LT：815QPS

![image-20230203213738275](https://tuchuang-1314129062.cos.ap-nanjing.myqcloud.com/images/202302032137322.png)

* Proactor，LT + ET：703QPS

![image-20230203213658380](https://tuchuang-1314129062.cos.ap-nanjing.myqcloud.com/images/202302032136427.png)

* Proactor，ET + LT：866QPS

![image-20230203213614572](https://tuchuang-1314129062.cos.ap-nanjing.myqcloud.com/images/202302032136617.png)

* Proactor，ET + ET：849QPS

![image-20230203213537189](https://tuchuang-1314129062.cos.ap-nanjing.myqcloud.com/images/202302032135250.png)

* Reactor，LT + LT：829QPS

![image-20230203213411838](https://tuchuang-1314129062.cos.ap-nanjing.myqcloud.com/images/202302032134886.png)

* Reactor，LT + ET：848QPS

![image-20230203213343254](https://tuchuang-1314129062.cos.ap-nanjing.myqcloud.com/images/202302032133301.png)

* Reactor，ET + LT：866QPS

![image-20230203213316257](https://tuchuang-1314129062.cos.ap-nanjing.myqcloud.com/images/202302032133310.png)

* Reactor，ET + ET：848QPS

![image-20230203213242806](https://tuchuang-1314129062.cos.ap-nanjing.myqcloud.com/images/202302032132859.png)



# 相关问题及解决方案

## 设计模式

### 单例模式

* **单例模式**：在整个项目中，**一个类仅有一个实例。**并提供一个访问该实例的全局访问点，该实例可以被项目中所有的程序模块共享；
* 实现思路：
  * 私有化该类的构造函数，防止外界创建单例类的对象；
  * 使用类的私有静态指针变量指向类的唯一实例，使用一个共有的静态方式获取该实例；
* 两种方式
  * 懒汉模式：不用的时候就不会初始化，该单例第一次被使用的时候才初始化；
  * 饿汉模式：在程序运行时就初始化；

### 生产者-消费者模式

* **生产者-消费者模式（阻塞队列）**
  * 当队列中没有元素时，对这个队列的弹出操作将会被阻塞，直到有元素被插入时才会被唤醒;
  * 当队列已满时，对这个队列的插入操作就会被阻塞，直到有元素被弹出后才会被唤醒;
* **生产者-消费者模型**：
  * 并发编程中的经典模型。以多线程为例，为了实现线程间数据同步，生产者线程与消费者线程**共享一个缓冲区**，其中**生产者线程往缓冲区中push消息，消费者线程从缓冲区中pop消息**。
  * 生产者和消费者是**互斥关系**，两者对缓冲区访问互斥，同时生产者和消费者又是一个相互协作与同步的关系，只有生产者生产之后，消费者才能消费。

## 大文件传输

* 每次传输后，都要更新下次传输的文件起始位置和长度

```cpp
// 解决大文件传输失败的问题
if(bytes_have_send >= m_iv[0].iov_len) {
    m_iv[0].iov_len = 0;
    m_iv[1].iov_base = m_file_address + (bytes_have_send - m_write_idx);
    m_iv[1].iov_len = bytes_to_send;
}
else {
    m_iv[0].iov_base = m_write_buf + bytes_have_send;
    m_iv[0].iov_len = m_iv[0].iov_len - bytes_have_send;
}
```

## 优雅关闭连接

* 优雅关闭：如果发送缓存区中还有数据未发出，则等其发送出去，并且收到所有数据的ACK之后，发送FIN包，开始关闭过程。TCP连接线关闭一个方向，此时另外一个方向还是可以正常进行数据传输。
* 强制关闭：如果缓存区中还有数据未发出，则这些数据都将被丢弃，然后发送RST包，直接重置TCP连接。两边都关闭了，服务端处理完的信息没有正常传给客户端。

设置优雅关闭连接的方法：在**setsocket函数中设置SO_LINGER**实现优雅关闭连接；

## 并发模型

* 半同步/半反应堆并发模型（半同步/半异步的变体）：
  * 同步：程序完全按照代码序列的顺序执行
  * 异步：程序的执行需要由系统事件驱动
* 半同步/半异步工作流程
  * 同步线程用于处理客户逻辑
  * 异步线程用于处理I/O事件
  * 异步线程监听到客户请求后，就将其封装成请求对象并插入请求队列中
  * 请求队列将通知某个工作在**同步模式的工作线程**来读取并处理该请求对象
* 半同步/半反应堆工作流程
  * 主线程充当异步线程，负责监听所有socket上的事件
  * 若有新请求到来，主线程接收之以得到新的连接socket，然后往epoll内核事件表中注册该socket上的读写事件
  * 如果连接socket上有读写事件发生，主线程从socket上接收数据，并将数据封装成请求对象插入到请求队列中
  * 所有工作线程睡眠在请求队列上，当有任务到来时，通过竞争（如互斥锁）获得任务的接管权

* 半同步/半反应堆模型的缺点与改进办法
  * 主线程与工作线程共享请求队列，主线程往请求队列中添加任务、工作线程从请求队列中取出任务都需要对请求队列**加锁保护**，耗费CPU时间；
  * 每个工作线程同一时间只能处理一个客户请求，当客户数量较多，工作线程数量较少时，请求队列会堆积任务，造成客户响应越来越慢。若通过增加工作线程解决，工作线程的切换也将耗费大量CPU时间；
  * **解决思路：使得每个工作线程能同时处理多个客户请求！**

## I/O复用

**使得程序能够同时监听多个文件描述符**。但是I/O复用本身是阻塞的，当多个文件描述符同时就绪时，如果不采取额外措施，程序只能按照顺序依次处理每一个文件描述符，这使得程序看起来像是串行工作的。如果要实现并发，只能采用多线程/多进程编程手段！

* **三种I/O复用方式比较**

**当监测的fd数量较小** ，且各个fd都很活跃的情况下，建议使用select和poll； **当监听的fd数量较多** ，且单位时间仅部分fd活跃的情况下，使用epoll会明显提升性能;

|                            |                        select                        |                        poll                        |                            epoll                             |
| :------------------------: | :--------------------------------------------------: | :------------------------------------------------: | :----------------------------------------------------------: |
|        底层数据结构        |                **数组**存储文件描述符                |               **链表**存储文件描述符               | **红黑树**存储监控的文件描述符，**双链表**存储就绪的文件描述符 |
| 如何从fd数据中获取就绪的fd |                      遍历fd_set                      |                      遍历链表                      |                             回调                             |
|         时间复杂度         |       获得就绪的文件描述符需要遍历fd数组，O(n)       |      获得就绪的文件描述符需要遍历fd链表，O(n)      | 当有就绪事件时，系统注册的回调函数就会被调用，将就绪的fd放入到就绪链表中。O(1) |
|         FD数据拷贝         | 每次调用select，需要将fd数据从用户空间拷贝到内核空间 | 每次调用poll，需要将fd数据从用户空间拷贝到内核空间 | 使用内存映射(mmap)，不需要从用户空间频繁拷贝fd数据到内核空间 |
|         最大连接数         |                  有限制，一般为1024                  |                       65535                        |                            65535                             |
|          工作模式          |                          LT                          |                         LT                         |                            ET，LT                            |

* **ET和LT**

**LT电平触发**：一个事件只要有，就会一直触发。epoll_wait检测到文件描述符有事件发生，则将其通知给应用程序，应用程序可以不立即处理该事件。
当下一次调用epoll_wait时，epoll_wait还会再次向应用程序报告此事件，直至被处理；

**ET边沿触发**：只有一个事件从无到有才会触发。epoll_wait检测到文件描述符有事件发生，则将其通知给应用程序，应用程序必须立即处理该事件，
必须要一次性将数据读取完，因为后续的epoll_wait将不会向应用程序通知这一事件；

**socket 的读事件**：对于LT模式，只要 socket 上有未读完的数据，就会一直产生 EPOLLIN 事件；而对于ET模式，socket 上每新来一次数据就会触发一次，如果上一次触发后，未将 socket 上的数据读完，也不会再触发，除非再新来一次数据。

**socket的写事件**：对于LT模式，如果 socket 的 TCP 窗口一直不饱和，会一直触发 EPOLLOUT 事件；而对于ET模式，只会触发一次，除非 TCP 窗口由不饱和变成饱和再一次变成不饱和，才会再次触发 EPOLLOUT 事件。

**LT和ET相比不容易遗漏事件，会一直提醒，而ET的性能会高**

**注意**：每个使用**ET模式的文件描述符必须是非阻塞的**，如果文件描述符是阻塞的，那么读操作或写操作都可能会因为没有后续事件而一直处于阻塞状态！

**EPOLLONESHOT**：一个线程读取某个socket上的数据后开始处理数据，在处理过程中该socket上又有新数据可读，此时另一个线程被唤醒读取，此时出现两个线程处理同一个socket。我们期望的是一个socket连接在任一时刻都只被一个线程处理，通过epoll_ctl对该文件描述符注册epolloneshot事件，一个线程处理socket时，其他线程将无法处理，当该线程处理完后，需要通过epoll_ctl重置epolloneshot事件，确保这个socket下次可读时，其EPOLLIN事件能被触发，让其他工作线程有机会继续处理这个socket！

## 数据存储：大端模式和小端模式

现代CPU的累加器一次能加载(至少)4个字节(32位机)的数据，这4个字节在内存中的**排列顺序**将影响它被累加器装载成的整数的值；

* 大端模式(**网络字节序**)：数据的低位保存在内存的高地址中，而数据的高位保存在内存的低地址中
* 小端模式(**主机字节序**)：数据的低位保存在内存的低地址中，而数据的高位保存在内存的高地址中
* 现代PC大多采用的是小端模式，socket网络传输时发送方采用大端模式
* **主机间传递数据时，发送方与接收方的统一方法**：发送方总是将发送的数据转换为大端字节序列，接收方根据自身来决定要不要转换字节序列

## 数据库优化

登录中的用户名和密码你是load到本地，然后使用map匹配的，如果有10亿数据，即使load到本地后hash，也是很耗时的，要怎么优化？

* 首先，将10亿的用户信息，利用大致缩小1000倍的hash算法进行hash，这时就获得了100万的hash数据，每一个hash数据代表着一个 **用户信息块（一级）**
* 而后，再分别对这100万的hash数据再进行hash，例如最终剩下1000个 **hash数据（二级）**
* 在这种方式下，服务器只需要保存1000个二级hash数据，当用户请求登录的时候，先对用户信息进行一次hash，找到对应信息块（二级），在读取其对应的一级信息块，最终找到对应的用户数据

